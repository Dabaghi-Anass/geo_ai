{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7485793,"sourceType":"datasetVersion","datasetId":4344943}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:13.794118Z","iopub.execute_input":"2025-06-09T21:58:13.794764Z","iopub.status.idle":"2025-06-09T21:58:13.798988Z","shell.execute_reply.started":"2025-06-09T21:58:13.794741Z","shell.execute_reply":"2025-06-09T21:58:13.798110Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/satellite-image-caption-generation/train.csv')\nimage_dir = '/kaggle/input/satellite-image-caption-generation'\n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:13.803050Z","iopub.execute_input":"2025-06-09T21:58:13.803277Z","iopub.status.idle":"2025-06-09T21:58:13.864794Z","shell.execute_reply.started":"2025-06-09T21:58:13.803256Z","shell.execute_reply":"2025-06-09T21:58:13.864096Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                                            captions               filepath\n0  ['Many aircraft are parked next to a long buil...    train/airport_1.jpg\n1  ['some planes are parked in an airport.'\\n 'th...   train/airport_10.jpg\n2  ['Many aircraft are parked in an airport near ...  train/airport_100.jpg\n3  ['Many aircraft are parked near a large buildi...  train/airport_101.jpg\n4  ['several buildings and green trees are around...  train/airport_102.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>captions</th>\n      <th>filepath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['Many aircraft are parked next to a long buil...</td>\n      <td>train/airport_1.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['some planes are parked in an airport.'\\n 'th...</td>\n      <td>train/airport_10.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['Many aircraft are parked in an airport near ...</td>\n      <td>train/airport_100.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['Many aircraft are parked near a large buildi...</td>\n      <td>train/airport_101.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['several buildings and green trees are around...</td>\n      <td>train/airport_102.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"class SatelliteDataset(Dataset):\n    def __init__(self, df, image_dir, transform, tokenizer, max_length=64):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        print(\"DataFrame columns:\", self.df.columns)\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['filepath'])  \n        caption = row['captions']                                   \n    \n        image = Image.open(image_path).convert('RGB')\n        pixel_values = self.transform(image)\n    \n        tokens = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n    \n        return {\n            'pixel_values': pixel_values,\n            'input_ids': tokens.input_ids.squeeze(),\n            'attention_mask': tokens.attention_mask.squeeze()\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:13.865933Z","iopub.execute_input":"2025-06-09T21:58:13.866145Z","iopub.status.idle":"2025-06-09T21:58:13.872185Z","shell.execute_reply.started":"2025-06-09T21:58:13.866131Z","shell.execute_reply":"2025-06-09T21:58:13.871330Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, AutoTokenizer\n\n# Load model and tokenizer\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nprocessor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# GPT2 has no pad_token by default, so set it to eos\ntokenizer.pad_token = tokenizer.eos_token\n\n# Set required config values\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:13.872901Z","iopub.execute_input":"2025-06-09T21:58:13.873226Z","iopub.status.idle":"2025-06-09T21:58:14.925805Z","shell.execute_reply.started":"2025-06-09T21:58:13.873202Z","shell.execute_reply":"2025-06-09T21:58:14.925066Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"add_cross_attention\": true,\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"decoder_start_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"is_decoder\": true,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"dataset = SatelliteDataset(df, image_dir, transform, tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:14.927479Z","iopub.execute_input":"2025-06-09T21:58:14.927698Z","iopub.status.idle":"2025-06-09T21:58:14.945232Z","shell.execute_reply.started":"2025-06-09T21:58:14.927681Z","shell.execute_reply":"2025-06-09T21:58:14.944407Z"}},"outputs":[{"name":"stdout","text":"DataFrame columns: Index(['captions', 'filepath'], dtype='object')\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nmodel.train()\nfor epoch in range(10):\n    for batch in dataloader:\n        pixel_values = batch['pixel_values'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        outputs = model(pixel_values=pixel_values, labels=input_ids)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:58:14.946523Z","iopub.execute_input":"2025-06-09T21:58:14.946715Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Loss: 2.0692\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.eval()\nimport matplotlib.pyplot as plt\n\nimage = Image.open('/kaggle/input/satellite-image-caption-generation/test/00628.jpg').convert(\"RGB\")\nplt.imshow(image)\nplt.show()\ninputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n# Génération\ngenerated_ids = model.generate(**inputs, max_length=64)\ncaption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# captions = caption.split(\"\\n\")\n# caption = \"\\n\".join([captions[0],captions[1]])\nprint(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"satellite-caption-model-v2\")\ntokenizer.save_pretrained(\"satellite-caption-model-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r satellite-caption-model-v2.zip /kaggle/working/satellite-caption-model-v2","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}